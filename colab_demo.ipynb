{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "colab_demo.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qU7OcMxPEs_c"
      },
      "source": [
        "# Entropic Desired Dynamics for Intrinsic ConTrol (EDDICT), a self-contained JAX implementation\n",
        "\n",
        "This is a simplified version of the code used in the EDDICT paper (to appear at NeurIPS 2021). In this stand-alone Google Colab, EDDICT is trained on a continuous grid world with an uncontrollable distractor. The resulting latent representations can then be seen to yield an interpretable model of the controllable aspects of the environment (i.e. the $(x,y)$ coordinates) while being invariant to the uncontrollable aspects (i.e. the distractor $(x,y)$ coordinates).\n",
        "\n",
        "## LICENSE\n",
        "\n",
        "Copyright 2021 DeepMind Technologies Limited.\n",
        "\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "https://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "l0y3jWd3sz1e"
      },
      "source": [
        "# @title (Optional) Install JAX 0.2.25 with CUDA support\n",
        "!pip install --upgrade pip\n",
        "# Installs the wheel compatible with CUDA 11 and cuDNN 8.2 or newer.\n",
        "!pip install --upgrade \"jax[cuda]==0.2.25\" -f https://storage.googleapis.com/jax-releases/jax_releases.html  # Note: wheels only available on linux."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "FZDJZViVLivE"
      },
      "source": [
        "#@title Imports\n",
        "import functools\n",
        "import dataclasses\n",
        "import datetime\n",
        "import math\n",
        "import operator\n",
        "\n",
        "import numpy as np\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import jit, grad, random, lax\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import Callable, NewType, Optional, Tuple, Union\n",
        "from typing_extensions import Protocol\n",
        "try:\n",
        "  import chex\n",
        "except ImportError:\n",
        "  !pip install git+git://github.com/deepmind/chex.git@v0.1.0\n",
        "  import chex\n",
        "try:\n",
        "  import haiku as hk\n",
        "except ImportError:\n",
        "  !pip install git+git://github.com/deepmind/dm-haiku.git@v0.0.5\n",
        "  import haiku as hk\n",
        "try:\n",
        "  import optax\n",
        "except ImportError:\n",
        "  !pip install git+git://github.com/deepmind/optax.git@v0.1.0\n",
        "  import optax\n",
        "try:\n",
        "  import rlax\n",
        "except ImportError:\n",
        "  !pip install git+git://github.com/deepmind/rlax.git@b652c45382605d3bf2c7db837364deda19819fce\n",
        "  import rlax\n",
        "\n",
        "\n",
        "try:\n",
        "  import jax.tools.colab_tpu\n",
        "  try:\n",
        "    jax.tools.colab_tpu.setup_tpu()\n",
        "  except KeyError:  # Not on a TPU Colab backend.\n",
        "    pass\n",
        "except ImportError:\n",
        "  pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "TkRfPuhPLn29"
      },
      "source": [
        "# @title Environment Dynamics\n",
        "\n",
        "# Define some type aliases. These don't actually help us in Colab, but if\n",
        "# we were to run through a type checker we'd get helpful errors if we tried\n",
        "# to pass actions where states were expected, etc.\n",
        "Actions = NewType('Action', chex.Array)\n",
        "States = NewType('State', chex.Array)\n",
        "LatentCodes = NewType('LatentCode', chex.Array)\n",
        "LatentCodeDeltas = NewType('LatentCodeDeltas', chex.Array)\n",
        "QValues = NewType('QValues', chex.Array)\n",
        "\n",
        "\n",
        "class Policy(Protocol):\n",
        "  \"\"\"Interface for an (unconditional) policy.\"\"\"\n",
        "\n",
        "  def __call__(self, rng_key: chex.PRNGKey, states: States) -> Actions:\n",
        "    \"\"\"Generate an action from the environment state and an RNG state.\n",
        "\n",
        "    Args:\n",
        "      rng_key: PRNGKey to use for any stochasticity in action selection.\n",
        "      states: A batch of environment states.\n",
        "    Returns:\n",
        "      A batch of integer actions, with shape equal to `states.shape[:-1]`.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "def sample_ball(\n",
        "  rng_key: chex.PRNGKey,\n",
        "  size: Union[int, chex.Shape],\n",
        "  dim: int = 2,\n",
        ") -> jnp.ndarray:\n",
        "  \"\"\"Sample from a uniform on the unit ball i.e. ||x|| <= 1.\n",
        "\n",
        "  Args:\n",
        "    rng_key: A JAX PRNGKey.\n",
        "    size: An integer or tuple of integers indicating the leading\n",
        "      dimensions (e.g. batch size).\n",
        "    dim: The dimension of the ball to generate within.\n",
        "\n",
        "  Returns:\n",
        "    An array with `size` as its leading dimension(s) and `dim` as its\n",
        "    final dimension where every `dim`-dimensional element is a sample\n",
        "    from the unit `dim`-ball.\n",
        "\n",
        "  See:\n",
        "    Muller, M.E. (1959). \"A note on a method for generating points\n",
        "    uniformly on n-dimensional spheres\". Communications of the\n",
        "    ACM, Volume 2 Issue 4, pp19-20.\n",
        "  \"\"\"\n",
        "  normal_key, unif_key = jax.random.split(rng_key, 2)\n",
        "  size = (size,) if isinstance(size, int) else size\n",
        "  direction = jax.random.normal(normal_key, size + (dim,))\n",
        "  norm = jnp.linalg.norm(direction, axis=-1, keepdims=True)\n",
        "  magnitude = jax.random.uniform(unif_key, size + (1,)) ** (1 / dim)\n",
        "  return direction / norm * magnitude\n",
        "\n",
        "\n",
        "@chex.dataclass(frozen=True)\n",
        "class Unroll:\n",
        "  \"\"\"The result of unrolling environment dynamics according to a policy.\n",
        "\n",
        "  `states` is expected to have one more element along its leading dimension\n",
        "  than `actions`, to account for the state reached after the last action.\n",
        "  \"\"\"\n",
        "  actions: Actions\n",
        "  states: States\n",
        "\n",
        "\n",
        "@chex.dataclass(frozen=True)\n",
        "class Environment:\n",
        "  \"\"\"Defines dynamics for a noisy point mass with distractor.\n",
        "\n",
        "  The state of the environment consists of 2 (x, y) pairs as a 4-vector.\n",
        "  the first pair are under the agent's control: the agent takes a step\n",
        "  in one of the 4 compass directions or 4 diagonal directions (or stays\n",
        "  still) with step length `action_size`, and is then displaced to a\n",
        "  point uniformly distributed in the circle of radius `noise_size`\n",
        "  centered on its position after applying the deterministic effect\n",
        "  of the chosen action. The distractor (x, y) coordinates are subject to a\n",
        "  a random walk under the same kind ofcircular noise but with a radius of\n",
        "  `action_size`; the agent's actions do not influence them whatsoever.\n",
        "\n",
        "  This class is stateless but for its configuration, and simply provides\n",
        "  methods to generate/mutate states.\n",
        "  \"\"\"\n",
        "  action_size: float = 0.1\n",
        "  noise_size: float = 0.2\n",
        "\n",
        "  STATE_DIM = 4\n",
        "  UNSCALED_ACTION_EFFECTS = np.array(\n",
        "      [[0., 0.],\n",
        "        [-1., 0.],\n",
        "        [0., -1.],\n",
        "        [1., 0.],\n",
        "        [0., 1.],\n",
        "        [-np.sqrt(.5), -np.sqrt(.5)],\n",
        "        [-np.sqrt(.5), np.sqrt(.5)],\n",
        "        [np.sqrt(.5), -np.sqrt(.5)],\n",
        "        [np.sqrt(.5), np.sqrt(.5)],\n",
        "      ],\n",
        "      dtype=np.float32,\n",
        "  )\n",
        "\n",
        "  @property\n",
        "  def num_actions(self) -> int:\n",
        "    \"\"\"Return the number of actions available in the environment.\"\"\"\n",
        "    return self.UNSCALED_ACTION_EFFECTS.shape[0]\n",
        "\n",
        "  def initialize(\n",
        "      self,\n",
        "      rng_key: chex.PRNGKey,\n",
        "      size: Union[int, chex.Shape],\n",
        "  ) -> jnp.ndarray:\n",
        "    \"\"\"Generate a set of initial states of the environment.\n",
        "\n",
        "    Args:\n",
        "      rng_key: A PRNGKey to use for random number generation.\n",
        "      size: An integer or tuple of integers indicating the leading\n",
        "        dimensions (e.g. batch size).\n",
        "    Returns:\n",
        "      An array with `size` as its leading dimension(s) and 4 as its\n",
        "      final dimension where every element along the final axis represents\n",
        "      an initial state of the environment drawn from the uniform\n",
        "      distribution on [-1, 1]^4.\n",
        "    \"\"\"\n",
        "    size = (size,) if isinstance(size, int) else size\n",
        "    return jax.random.uniform(\n",
        "        rng_key,\n",
        "        size + (self.STATE_DIM,),\n",
        "        minval=-1,\n",
        "        maxval=1,\n",
        "    )\n",
        "\n",
        "  def transition(\n",
        "      self,\n",
        "      rng_key: chex.PRNGKey,\n",
        "      states: States,\n",
        "      actions: Actions,\n",
        "      stop_gradient: bool = True,\n",
        "  ) -> States:\n",
        "    \"\"\"Step the environment dynamics.\n",
        "\n",
        "    Args:\n",
        "      rng_key: A PRNGKey to use for random number generation.\n",
        "      states: An array representing the current state of the environment.\n",
        "      actions: An array of actions with one less dimension than `states`\n",
        "        and the same leading dimensions representing actions to be taken.\n",
        "      stop_gradient: Boolean indicating whether to wrap resulting states\n",
        "        in a `jax.lax.stop_gradient` so that the environment cannot be\n",
        "        differentiated through (defaults to True).\n",
        "    Returns:\n",
        "      A new set of states with the same shape as `states`, resulting from\n",
        "      applying `actions` and advancing the dynamics one step.\n",
        "    \"\"\"\n",
        "    chex.assert_axis_dimension(states, -1, Environment.STATE_DIM)\n",
        "    chex.assert_equal_shape_prefix([states, actions], actions.ndim)\n",
        "\n",
        "    # Sample from the ball twice for each batch member and reshape to\n",
        "    # horizontally stack pairs beside each other.\n",
        "    noise = (\n",
        "        sample_ball(rng_key, (2,) + actions.shape, 2)\n",
        "        .reshape(actions.shape + (4,))\n",
        "    )\n",
        "    # Scale state dimensions 0 and 1 by noise_size, 2 and 3 by action_size.\n",
        "    noise_gain = jnp.array([self.noise_size] * 2 + [self.action_size] * 2)\n",
        "\n",
        "    # New state is the sum of action effects and noise.\n",
        "    all_action_effects = self.action_size * self.UNSCALED_ACTION_EFFECTS\n",
        "\n",
        "    # Add zeros for distractor dimensions.\n",
        "    action_effect = jnp.concatenate([\n",
        "      all_action_effects[actions],\n",
        "      jnp.zeros_like(all_action_effects[actions]),\n",
        "    ], axis=-1)\n",
        "    displacement = action_effect + noise * noise_gain\n",
        "    s_prime = jnp.clip(states + displacement, -1, 1)\n",
        "\n",
        "    return jax.lax.select(stop_gradient, lax.stop_gradient(s_prime), s_prime)\n",
        "\n",
        "  @functools.partial(jax.jit, static_argnums=1)\n",
        "  def unroll(\n",
        "      self,\n",
        "      policy: Policy,\n",
        "      rng_keys: chex.PRNGKey,\n",
        "      initial: States,\n",
        "      stop_gradient: bool = True,\n",
        "  ) -> Unroll:\n",
        "    \"\"\"Unroll a trajectory from an initial state according to a policy.\n",
        "\n",
        "    Args:\n",
        "      policy: A callable taking a PRNGKey and a batch of states of the\n",
        "        environment and returning actions.\n",
        "      rng_keys: A pre-split PRNGKey with leading dimension equal to the\n",
        "        length of the desired trajectory.\n",
        "      initial: A batch of initial states for the trajectories.\n",
        "      stop_gradient: A boolean indicating whether to place a `stop_gradient`\n",
        "        around the environment state so that it cannot be differentiated\n",
        "        (defaults to True).\n",
        "    Returns:\n",
        "      An Unroll containing `len(rng_keys)` actions and `len(rng_keys) + 1`\n",
        "      states for each member of the batch.\n",
        "    \"\"\"\n",
        "    def loop_body(\n",
        "        states: States,\n",
        "        key: chex.PRNGKey,\n",
        "    ) -> Tuple[States, Unroll]:\n",
        "      \"\"\"Samples an action from the policy, step the environment dynamics.\n",
        "\n",
        "      Args:\n",
        "        states: The current state(s) of the environment.\n",
        "        key: The PRNGKey to use for this step.\n",
        "\n",
        "      Returns:\n",
        "        A tuple of the next state(s) of the environment and an `Unroll` pair\n",
        "        containing `states` and the action(s) sampled from it.\n",
        "      \"\"\"\n",
        "      action_key, state_key = jax.random.split(key)\n",
        "      actions = policy(action_key, states)\n",
        "      new_states = self.transition(state_key, states, actions, stop_gradient)\n",
        "\n",
        "      # N.B. states returned as part of unroll are the ones passed in as an\n",
        "      # argument. The new state is only passed to the next iteration.\n",
        "      return new_states, Unroll(actions=actions, states=states)\n",
        "\n",
        "    # We will want to concatenate the final state with the unroll, and thus\n",
        "    # we will end up with one more state than action.\n",
        "    final, unroll = jax.lax.scan(loop_body, initial, rng_keys)\n",
        "    all_states = jnp.concatenate([unroll.states, final[jnp.newaxis]])\n",
        "    return unroll.replace(states=all_states)\n",
        "\n",
        "\n",
        "@dataclasses.dataclass(frozen=True)\n",
        "class UniformRandomPolicy:\n",
        "  \"\"\"A trivial random uniform policy for the above environment.\n",
        "\n",
        "  Defined as a dataclass so that the hash depends only on the number of\n",
        "  actions. Defined in the cell above the plotting code so that the class\n",
        "  isn't redefined every time the cell is refreshed, which would invalidate\n",
        "  the JIT compile cache for `env.unroll`.\n",
        "  \"\"\"\n",
        "  num_actions: int\n",
        "\n",
        "  def __call__(self, rng_key: chex.PRNGKey, states: States) -> Actions:\n",
        "    action_shape = states.shape[:-1]\n",
        "    return jax.random.randint(rng_key, action_shape, 0, self.num_actions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "wgz-_ZWLO8rq"
      },
      "source": [
        "#@title Plot an example trajectory (uniform random policy) { run: \"auto\" }\n",
        "\n",
        "trajectory_seed = 0  # @param { 'type': 'slider' }\n",
        "trajectory_length = 1000  # @param { 'type': 'slider', 'min': 100, 'max': 2000, 'step': 100 }\n",
        "trajectory_action_size = 0.1  # @param { 'type': 'slider', 'min': 0.1, 'max': 0.5, 'step': 0.01 }\n",
        "trajectory_noise_size = 0.2  # @param { 'type': 'slider', 'min': 0.0, 'max': 0.5, 'step': 0.01 }\n",
        "\n",
        "\n",
        "def plot_example_trajectory():\n",
        "  \"\"\"Plot an example trajectory from the hyperparameters specified above.\"\"\"\n",
        "  env = Environment(\n",
        "      action_size=trajectory_action_size,\n",
        "      noise_size=trajectory_noise_size,\n",
        "  )\n",
        "  keys = jax.random.split(\n",
        "      jax.random.PRNGKey(trajectory_seed),\n",
        "      trajectory_length + 1,\n",
        "  )\n",
        "  initial = env.initialize(keys[0], ())\n",
        "  # N.B. This will re-jit for different values of trajectory_length. Everything\n",
        "  # else should be fast to recompute.\n",
        "\n",
        "  policy = UniformRandomPolicy(env.num_actions)\n",
        "  trajectory = env.unroll(policy, keys[1:], initial)\n",
        "\n",
        "  x, y = trajectory.states.T[:2]\n",
        "  plt.scatter(x, y, c=np.arange(trajectory_length + 1), linewidths=1)\n",
        "  plt.title(\"Trajectory in controllable state dimensions (color = time)\")\n",
        "\n",
        "plot_example_trajectory()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "aJvFN8LCVcLd"
      },
      "source": [
        "# @title Network definitions\n",
        "\n",
        "class ConditionalQNetwork(Protocol):\n",
        "  \"\"\"Interface for a (conditional) Q-network.\"\"\"\n",
        "  def __call__(self, state: States, desired_z: LatentCodes) -> QValues:\n",
        "    \"\"\"Compute Q-values (action values) for an environment state and code.\n",
        "\n",
        "    Args:\n",
        "      rng_key: PRNGKey to use for any stochasticity in action selection.\n",
        "      states: A (batch of) environment state(s).\n",
        "    Returns:\n",
        "      A (batch of) Q-values, with leading dimensions `states.shape[:-1]`\n",
        "      and a final axis with size equal to the number of environment actions.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "class CodePredictor(Protocol):\n",
        "  \"\"\"Interface for a mapping from environment states to latent codes.\"\"\"\n",
        "  def __call__(self, states: States) -> LatentCodes:\n",
        "    \"\"\"Predict a (batch of) latent code(s) from a (batch of) environment states.\n",
        "\n",
        "    Args:\n",
        "      states: A (batch of) environment state(s).\n",
        "    Returns:\n",
        "      A (batch of) latent code(s) corresponding to the environment state(s).\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "@dataclasses.dataclass(frozen=True)\n",
        "class Networks:\n",
        "  \"\"\"A bundle of a Q network and a latent code predictor.\"\"\"\n",
        "  q_network: hk.Transformed\n",
        "  predictor: hk.Transformed\n",
        "\n",
        "  @classmethod\n",
        "  def build(cls, env: Environment, hid_dim: int, code_dim: int) -> 'Networks':\n",
        "    \"\"\"Build a pair of networks from an environment and hyperparameters.\"\"\"\n",
        "\n",
        "    def predictor(states: States) -> LatentCodes:\n",
        "      \"\"\"A simple multilayer perceptron that maps states to codes.\"\"\"\n",
        "      chex.assert_axis_dimension(states, -1, Environment.STATE_DIM)\n",
        "      return hk.nets.MLP(\n",
        "          [hid_dim, hid_dim, code_dim],\n",
        "          name='predictor',\n",
        "      )(states)\n",
        "\n",
        "    def q_network(state: States, desired_z: LatentCodes) -> QValues:\n",
        "      \"\"\"A Q-network that concatenates a desired latent vector.\"\"\"\n",
        "      chex.assert_axis_dimension(state, -1, Environment.STATE_DIM)\n",
        "      chex.assert_axis_dimension(desired_z, -1, code_dim)\n",
        "\n",
        "      if state.ndim > desired_z.ndim:\n",
        "        # Tile desired_z along leading dimensions when doing batched evaluation.\n",
        "        leading_dims = state.ndim - desired_z.ndim\n",
        "        tiling = state.shape[:leading_dims] + (1,) * desired_z.ndim\n",
        "        desired_z = jnp.tile(desired_z, tiling)\n",
        "      return hk.nets.MLP(\n",
        "          [hid_dim, hid_dim, env.num_actions],\n",
        "          name='q_network',\n",
        "      )(jnp.concatenate([state, desired_z], axis=-1))\n",
        "\n",
        "    return cls(\n",
        "        # Our networks are deterministic so remove the RNG argument from apply.\n",
        "        q_network=hk.without_apply_rng(hk.transform(q_network)),\n",
        "        predictor=hk.without_apply_rng(hk.transform(predictor)),\n",
        "    )\n",
        "\n",
        "  def init(self, rng_key: chex.PRNGKey) -> hk.Params:\n",
        "    \"\"\"Initialize both networks, return a joint parameters container.\n",
        "\n",
        "    Args:\n",
        "      rng_key: A PRNGKey.\n",
        "    Returns:\n",
        "      A `hk.Params` containing the parameters for both the Q-network\n",
        "      and the predictor, which can be passed to either `apply` function.\n",
        "    \"\"\"\n",
        "    p_key, q_key = jax.random.split(rng_key)\n",
        "\n",
        "    # We know that the states will always be the same dimension.\n",
        "    states = jnp.zeros(Environment.STATE_DIM)\n",
        "\n",
        "    # Initialize the predictor first, so we can encode our dummy state\n",
        "    # and states.\n",
        "    predictor_params = self.predictor.init(p_key, states)\n",
        "\n",
        "    # Encode to get a code value, and use it to initialize the Q network.\n",
        "    z = self.predictor.apply(predictor_params, states)\n",
        "    q_network_params = self.q_network.init(q_key, states, z)\n",
        "\n",
        "    # Merge together the parameter containers. The namespaces do not overlap\n",
        "    # so this will work with either transformed functions.\n",
        "    return hk.data_structures.merge(predictor_params, q_network_params)\n",
        "\n",
        "  def with_params(\n",
        "      self,\n",
        "      params: hk.Params\n",
        "  ) -> Tuple[ConditionalQNetwork, CodePredictor]:\n",
        "    \"\"\"Return callables that curry (cache) a set of parameters for convenience.\n",
        "\n",
        "    Args:\n",
        "      params: A Haiku parameter set.\n",
        "    Returns:\n",
        "      A pair of `functools.partial` objects for the Q-network and predictor,\n",
        "      respectively, that respect the `ConditionalQNetwork` and `CodePredictor`\n",
        "      interfaces defined above.\n",
        "    \"\"\"\n",
        "    return tuple(functools.partial(getattr(self, n).apply, params)\n",
        "                 for n in ('q_network', 'predictor'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "-FyXDpDEU3SA"
      },
      "source": [
        "# @title Actor Loop\n",
        "\n",
        "@dataclasses.dataclass(frozen=True)\n",
        "class TrainingConfig:\n",
        "  \"\"\"All of the constant configuration that doesn't change during training.\n",
        "\n",
        "  Bundle this together so that our top-level functions don't have a dozen\n",
        "  arguments and we don't rely on a cluttered global namespace, which leads to\n",
        "  confusing bugs.\n",
        "  \"\"\"\n",
        "  nets: Networks\n",
        "  optimizer: optax.GradientTransformation\n",
        "  env: Environment\n",
        "  code_dim: int\n",
        "  gamma: float\n",
        "  goal_duration: int\n",
        "  train_epsilon: float\n",
        "  evaluation_epsilon: float\n",
        "  evaluation_batch_size: int\n",
        "\n",
        "\n",
        "@chex.dataclass(frozen=True)\n",
        "class TrainingState:\n",
        "  \"\"\"All of the state that _does_ change during training.\"\"\"\n",
        "  env_state: States\n",
        "  params: hk.Params\n",
        "  opt_state: optax.OptState\n",
        "  rng_key: chex.PRNGKey\n",
        "\n",
        "\n",
        "@chex.dataclass(frozen=True)\n",
        "class Batch:\n",
        "  \"\"\"A batch of data generated by sampling code deltas and acting.\"\"\"\n",
        "  actions: Actions\n",
        "  states: States\n",
        "  deltas: LatentCodeDeltas\n",
        "\n",
        "\n",
        "def get_batch(\n",
        "    env: Environment,\n",
        "    nets: Networks,\n",
        "    state: TrainingState,\n",
        "    goal_duration: int,\n",
        "    epsilon: float,\n",
        ") -> Batch:\n",
        "  \"\"\"Act in the provided environment using the given network.\n",
        "\n",
        "  Args:\n",
        "    env: The `Environment` instance defining the dynamics.\n",
        "    nets: A `Networks` bundle of a Q-network and predictor.\n",
        "    state: The current `TrainingState`.\n",
        "    goal_duration: The number of actions to take in the environment\n",
        "      per goal period.\n",
        "    epsilon: Value to use for epsilon-greedy exploration.\n",
        "  Returns:\n",
        "    A `Batch` of unrolled states, actions and the code deltas added\n",
        "    to the encoded initial state and used to condition the Q network when\n",
        "    acting.\n",
        "  \"\"\"\n",
        "  q_network, predictor = nets.with_params(state.params)\n",
        "  chex.assert_rank(state.env_state, 2)\n",
        "  batch_size = state.env_state.shape[0]\n",
        "\n",
        "  # Split one key per step, plus one for sampling deltas.\n",
        "  keys = jax.random.split(state.rng_key, goal_duration + 1)\n",
        "  delta_key, step_keys = keys[0], keys[1:]\n",
        "\n",
        "  initial_z = predictor(state.env_state)\n",
        "  deltas = sample_ball(delta_key, batch_size, initial_z.shape[-1])\n",
        "  desired_z = initial_z + deltas\n",
        "\n",
        "  def batched_epsilon_greedy(key: chex.PRNGKey, states: jnp.ndarray) -> Actions:\n",
        "    \"\"\"Execute an epsilon greedy behavior policy conditioned on `desired_z`.\n",
        "\n",
        "    RLax epsilon greedy is unbatched, meaning we have to split the RNG key and\n",
        "    use `jax.vmap`.\n",
        "    \"\"\"\n",
        "    chex.assert_rank(states, 2)\n",
        "    batch_keys = jax.random.split(key, batch_size)\n",
        "    q_values = q_network(states, desired_z)\n",
        "    return jax.vmap(rlax.epsilon_greedy(epsilon).sample)(batch_keys, q_values)\n",
        "\n",
        "  unroll = env.unroll(batched_epsilon_greedy, step_keys, state.env_state)\n",
        "\n",
        "  # Chex dataclasses implementing the mapping interface by default, so we can\n",
        "  # use them just like dicts with the ** operator.\n",
        "  return Batch(deltas=deltas, **unroll)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "gGuH-J_KGg66"
      },
      "source": [
        "#@title Loss\n",
        "\n",
        "PMAP_AXIS = 'devices'\n",
        "\n",
        "@chex.dataclass(frozen=True)\n",
        "class Statistics:\n",
        "  \"\"\"Statistics computed along with the loss.\"\"\"\n",
        "  value_loss: float\n",
        "  predictor_loss: float\n",
        "  total_loss: float\n",
        "\n",
        "\n",
        "def eddict_loss(\n",
        "    nets: Networks,\n",
        "    params: hk.Params,\n",
        "    batch: Batch,\n",
        "    gamma: float,\n",
        ") -> Tuple[float, Statistics]:\n",
        "  \"\"\"Loss function for EDDICT.\n",
        "\n",
        "  The EDDICT loss is actually a sum of two losses that operate on\n",
        "  disjoint parameter sets, treated as one loss here for convenience.\n",
        "  \"\"\"\n",
        "  chex.assert_rank([batch.actions, batch.deltas, batch.states], [2, 2, 3])\n",
        "  chex.assert_equal_shape_prefix([batch.states[:-1], batch.actions], 2)\n",
        "\n",
        "  q_network, predictor = nets.with_params(params)\n",
        "\n",
        "  desired_z = predictor(batch.states[0]) + batch.deltas\n",
        "  q_values = q_network(batch.states, lax.stop_gradient(desired_z))\n",
        "  achieved_z = predictor(batch.states[-1])\n",
        "\n",
        "  # The squared Euclidean distance between the conditioned and desired codes\n",
        "  # serve as the loss for the predictor and (negated) reward for the policy.\n",
        "  code_errors = jnp.square(achieved_z - desired_z).sum(axis=-1)\n",
        "\n",
        "  # Give a reward equal to the negative squared code error at the last step.\n",
        "  num_transitions = batch.actions.shape[0]\n",
        "  reward_mask = jnp.arange(num_transitions) == (num_transitions - 1)\n",
        "  rewards = jnp.outer(reward_mask, -code_errors)\n",
        "\n",
        "  # Do not bootstrap from the final state (discount 0).\n",
        "  # Discounts are the same for every batch item, so this has no batch dimension.\n",
        "  discounts = (1 - reward_mask) * gamma\n",
        "\n",
        "  # vmap twice, over time and inside that over batch.\n",
        "  q_tm1, q_t = q_values[:-1], q_values[1:]\n",
        "  td_errors = jax.vmap(\n",
        "      # Don't vmap the batch dimension over discounts (which don't have one).\n",
        "      jax.vmap(rlax.q_learning, in_axes=[0, 0, 0, None, 0]),\n",
        "  )(q_tm1, batch.actions, rewards, discounts, q_t)\n",
        "\n",
        "  value_loss = jnp.square(td_errors).sum(axis=0).mean()\n",
        "  predictor_loss = code_errors.mean()\n",
        "\n",
        "  stats = Statistics(\n",
        "      value_loss=value_loss,\n",
        "      predictor_loss=predictor_loss,\n",
        "      total_loss=value_loss + predictor_loss,\n",
        "  )\n",
        "  # Average the loss across devices.\n",
        "  stats = jax.lax.pmean(stats, PMAP_AXIS)\n",
        "  return stats.total_loss, stats"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "dAlCEtG1P6vE"
      },
      "source": [
        "# @title Training Step\n",
        "\n",
        "@functools.partial(\n",
        "    jax.pmap,\n",
        "    axis_name=PMAP_AXIS,\n",
        "    static_broadcasted_argnums=0,\n",
        "    donate_argnums=1,\n",
        ")\n",
        "def training_step(\n",
        "    config: TrainingConfig,\n",
        "    state: TrainingState,\n",
        ") -> TrainingState:\n",
        "  \"\"\"Do a full training iteration: sample a data batch and update parameters.\"\"\"\n",
        "  rng_key, next_key = jax.random.split(state.rng_key)\n",
        "  batch = get_batch(\n",
        "      config.env,\n",
        "      config.nets,\n",
        "      state.replace(rng_key=rng_key),\n",
        "      config.goal_duration,\n",
        "      config.train_epsilon,\n",
        "  )\n",
        "\n",
        "  grad_loss = jax.grad(eddict_loss, argnums=1, has_aux=True)\n",
        "  dev_grads, _ = grad_loss(config.nets, state.params, batch, config.gamma)\n",
        "\n",
        "  # The gradient of an average should be the average of the gradients.\n",
        "  grads = jax.lax.pmean(dev_grads, PMAP_AXIS)\n",
        "  updates, new_opt_state = config.optimizer.update(grads, state.opt_state)\n",
        "  new_params = optax.apply_updates(state.params, updates)\n",
        "\n",
        "  return state.replace(\n",
        "      params=optax.apply_updates(state.params, updates),\n",
        "      env_state=batch.states[-1],  # Next step starts from last batch of states.\n",
        "      opt_state=new_opt_state,\n",
        "      rng_key=next_key,\n",
        "  )\n",
        "\n",
        "\n",
        "@functools.partial(\n",
        "    jax.pmap,\n",
        "    axis_name=PMAP_AXIS,\n",
        "    static_broadcasted_argnums=0,\n",
        "    donate_argnums=1,\n",
        ")\n",
        "def evaluate(\n",
        "    config: TrainingConfig,\n",
        "    state: TrainingState,\n",
        ") -> Statistics:\n",
        "  \"\"\"Gather a large batch of trajectories and evaluate diagnostics.\"\"\"\n",
        "  # We split the current key but don't replace it in the TrainingState.\n",
        "  # This means the training results are independent of how often we evaluate.\n",
        "  env_key, act_key = jax.random.split(state.rng_key)\n",
        "  env_state = config.env.initialize(env_key, config.evaluation_batch_size)\n",
        "  batch = get_batch(\n",
        "      config.env,\n",
        "      config.nets,\n",
        "      # Substitute the batch of evaluation start states, and the RNG key\n",
        "      # we just split. Note that this object isn't returned to the training\n",
        "      # loop so this doesn't affect how the RNG evolves for training purposes,\n",
        "      # and our training results should be the same independent of how often\n",
        "      # we evaluate.\n",
        "      state.replace(env_state=env_state, rng_key=act_key),\n",
        "      config.goal_duration,\n",
        "      config.evaluation_epsilon,\n",
        "  )\n",
        "  _, stats = eddict_loss(config.nets, state.params, batch, config.gamma)\n",
        "  return stats\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "lZHMjQkxVFUR"
      },
      "source": [
        "#@title Network & Environment Setup (changing these settings will also reinitialize parameters) { run: \"auto\" }\n",
        "\n",
        "# EDDICT goal duration.\n",
        "train_goal_duration = 10  # @param {'type': 'slider', 'min': 2, 'max': 100}\n",
        "\n",
        "# RL hyperparameters.\n",
        "train_epsilon = 0.1  # @param {'type': 'slider', 'min': 0.05, 'max': 0.5, 'step': 0.05}\n",
        "train_gamma = 0.99  # @param {'type': 'slider', 'min': 0.95, 'max': 0.999, 'step': 0.001}\n",
        "\n",
        "# Network hyperparameters.\n",
        "train_net_hid_dim = 128  # @param {'type': 'integer'}\n",
        "train_net_code_dim = 2  # The visualizations are based on 2-dimensional codes.\n",
        "\n",
        "# Environment hyperparameters.\n",
        "train_env_action_size = 0.1  # @param { 'type': 'slider', 'min': 0.1, 'max': 0.9, 'step': 0.1 }\n",
        "train_env_noise_size = 0.2  # @param { 'type': 'slider', 'min': 0.1, 'max': 0.9, 'step': 0.1 }\n",
        "\n",
        "# Optimizer hyperparameters.\n",
        "train_batch_size_per_device = 256  # @param\n",
        "train_adam_learning_rate = 1e-4  # @param\n",
        "train_adam_eps = 1e-2  # @param\n",
        "\n",
        "# Define the initialization seed separately so that it can vary independently.\n",
        "train_initialization_seed = 0  # @param {'type': 'integer'}\n",
        "train_seed = 1  # @param {'type': 'integer'}\n",
        "\n",
        "# Evaluation hyperparameters.\n",
        "evaluation_epsilon = 0.1  # @param {'type': 'slider', 'min': 0.05, 'max': 0.5, 'step': 0.05}\n",
        "evaluation_batch_size_per_device = 1024  # @param {'type': 'integer'}\n",
        "\n",
        "\n",
        "def _build_config() -> TrainingConfig:\n",
        "  \"\"\"Put this in a function so we have less stuff in global namespace.\"\"\"\n",
        "  env = Environment(\n",
        "      action_size=train_env_action_size,\n",
        "      noise_size=train_env_noise_size,\n",
        "  )\n",
        "  nets = Networks.build(env, train_net_hid_dim, train_net_code_dim)\n",
        "  optimizer = optax.adam(train_adam_learning_rate, eps=train_adam_eps)\n",
        "  return TrainingConfig(\n",
        "      nets=nets,\n",
        "      optimizer=optimizer,\n",
        "      env=env,\n",
        "      code_dim=train_net_code_dim,\n",
        "      gamma=train_gamma,\n",
        "      goal_duration=train_goal_duration,\n",
        "      train_epsilon=train_epsilon,\n",
        "      evaluation_epsilon=evaluation_epsilon,\n",
        "      evaluation_batch_size=evaluation_batch_size_per_device,\n",
        "  )\n",
        "\n",
        "def _construct_state(\n",
        "    config: TrainingConfig,\n",
        ") -> TrainingState:\n",
        "  env_key, train_rng_key = [\n",
        "      jax.random.split(k, jax.local_device_count())\n",
        "      for k in jax.random.split(jax.random.PRNGKey(train_seed))\n",
        "  ]\n",
        "\n",
        "  init_key = jax.random.PRNGKey(train_initialization_seed)\n",
        "\n",
        "  params = jax.pmap(config.nets.init)(\n",
        "      jax.device_put_replicated(init_key, jax.local_devices())\n",
        "  )\n",
        "  return TrainingState(\n",
        "      env_state=jax.pmap(\n",
        "          config.env.initialize,\n",
        "          static_broadcasted_argnums=1,\n",
        "      )(env_key, train_batch_size_per_device),\n",
        "      params=params,\n",
        "      opt_state=jax.pmap(config.optimizer.init)(params),\n",
        "      rng_key=train_rng_key,\n",
        "  )\n",
        "\n",
        "train_config = _build_config()\n",
        "train_state = _construct_state(train_config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "0Dw5zkloulRn"
      },
      "source": [
        "#@title Training Loop\n",
        "\n",
        "train_iterations = 10000  # @param {'type': 'integer'}\n",
        "evaluation_period = 1000  # @param {'type': 'integer'}\n",
        "\n",
        "\n",
        "def log_statistics(iteration: int, stats: Statistics, spaces: int = 2):\n",
        "  stats = dict(jax.tree_map(operator.itemgetter(0), jax.device_get(stats)))\n",
        "  print(\n",
        "      'Iteration ',\n",
        "      str(iteration).rjust(math.ceil(1 + math.log10(train_iterations))),\n",
        "      (' ' * spaces).join(\n",
        "          f'{k} = %9.7f' % v\n",
        "          for k, v in stats.items()\n",
        "      )\n",
        "  )\n",
        "\n",
        "\n",
        "print(('Training with goal duration %d, batch size %d on %d devices '\n",
        "       '(total per-step batch size %d, %d transitions per batch)') %\n",
        "      (train_goal_duration, train_batch_size_per_device,\n",
        "       jax.local_device_count(),\n",
        "       train_batch_size_per_device * jax.local_device_count(),\n",
        "       (jax.local_device_count() * train_batch_size_per_device *\n",
        "        train_goal_duration)))\n",
        "print('Evaluating on a batch of size %d per device '\n",
        "      '(for a total of %d trajectories per evaluation)' %\n",
        "      (evaluation_batch_size_per_device,\n",
        "       evaluation_batch_size_per_device * jax.local_device_count()))\n",
        "\n",
        "\n",
        "train_stats = evaluate(train_config, train_state)\n",
        "log_statistics(0, train_stats)\n",
        "print('Beginning training.')\n",
        "t_start = datetime.datetime.now()\n",
        "for i in range(train_iterations):\n",
        "  train_state = training_step(train_config, train_state)\n",
        "  if (i + 1) % evaluation_period == 0:\n",
        "    train_stats = evaluate(train_config, train_state)\n",
        "    log_statistics(i + 1, train_stats)\n",
        "\n",
        "t_end = datetime.datetime.now()\n",
        "print(f'Training took {t_end - t_start}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "2Ve5Vn1Gi59B"
      },
      "source": [
        "#@title Visualize Results\n",
        "\n",
        "def plot_results(\n",
        "    config: TrainingConfig,\n",
        "    train_state: TrainingState,\n",
        "    grid_distractors: bool = False,\n",
        "):\n",
        "  # meshgrid over 2 dimensions, random sampling for the other 2\n",
        "  if grid_distractors:\n",
        "    point_grid = np.meshgrid(np.linspace(-1.,1., dtype=np.float32), np.linspace(-1.,1., dtype=np.float32))\n",
        "    points = np.hstack(\n",
        "        [np.random.uniform(-1, 1, size=points.shape),\n",
        "         np.stack(point_grid, -1).reshape((-1, 2))]\n",
        "    )\n",
        "  else:\n",
        "    point_grid = np.meshgrid(\n",
        "      np.linspace(-1.,1., dtype=np.float32),\n",
        "      np.linspace(-1.,1., dtype=np.float32),\n",
        "    )\n",
        "    points = np.stack(point_grid, -1).reshape([-1, 2])\n",
        "    points = np.concatenate(\n",
        "        [points, np.random.uniform(-1, 1, size=points.shape)],\n",
        "        axis=-1,\n",
        "    )\n",
        "  states = points\n",
        "  delta = np.random.uniform(size=(states.shape[0], train_config.code_dim))\n",
        "\n",
        "  # Retrieve a single copy of the parameters.\n",
        "  params = jax.tree_map(\n",
        "      operator.itemgetter(0),\n",
        "      jax.device_get(train_state.params),\n",
        "  )\n",
        "  z = config.nets.predictor.apply(params, states)\n",
        "\n",
        "  plt.figure(figsize=[20, 5])\n",
        "  plt.suptitle(\"EDDICT $z$ vs $(x,y)$ position\", y=1.05)\n",
        "  plt.subplot(1, 4, 1)\n",
        "  plt.title(\"$z_0$ vs $z_1$ \\n color=$x$\")\n",
        "  plt.axis('off')\n",
        "  plt.scatter(z[:,0], z[:,1], c=states[:,0])\n",
        "  plt.subplot(1, 4, 2)\n",
        "  plt.title(\"$z_0$ vs $z_1$ \\n color=$y$\")\n",
        "  plt.axis('off')\n",
        "  plt.scatter(z[:,0], z[:,1], c=states[:,1])\n",
        "  plt.subplot(1, 4, 3)\n",
        "  plt.title(\"$x$ vs $y$ \\n color=$z_0$\")\n",
        "  plt.axis('off')\n",
        "  plt.scatter(states[:,0], states[:,1], c=z[:,0])\n",
        "  plt.subplot(1, 4, 4)\n",
        "  plt.title(\"$x$ vs $y$ \\n color=$z_1$\")\n",
        "  plt.axis('off')\n",
        "  plt.scatter(states[:,0], states[:,1], c=z[:,1])\n",
        "\n",
        "  plt.figure(figsize=[20, 5])\n",
        "  plt.suptitle(\"EDDICT Z vs Uncontrollable Distractor $(x,y)$ position\", y=1.05)\n",
        "  plt.subplot(1, 4, 1)\n",
        "  plt.title(\"$z_0$ vs $z_1$ \\n color=distractor x\")\n",
        "  plt.axis('off')\n",
        "  plt.scatter(z[:,0], z[:,1], c=states[:,2])\n",
        "  plt.subplot(1, 4, 2)\n",
        "  plt.title(\"$z_0$ vs $z_1$ \\n color=distractor y\")\n",
        "  plt.axis('off')\n",
        "  plt.scatter(z[:,0], z[:,1], c=states[:,3])\n",
        "  plt.subplot(1, 4, 3)\n",
        "  plt.title(\"distractor $x$ vs $y$ \\n color=$z_0$\")\n",
        "  plt.axis('off')\n",
        "  plt.scatter(states[:,2], states[:,3], c=z[:,0])\n",
        "  plt.subplot(1, 4, 4)\n",
        "  plt.title(\"distractor $x$ vs $y$ \\n color=$z_1$\")\n",
        "  plt.axis('off')\n",
        "  plt.scatter(states[:,2], states[:,3], c=z[:,1])\n",
        "\n",
        "plot_results(train_config, train_state)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
